---
title: "Statistical Analysis Continued"
format: html
---

# Model Evaluation

## Loading Data and Packages

Our first step is to import our model data and load in all necessary test data frames for evaluating model success. Additionally we'll set the seed for the entirety of this document for the purposes of ensuring the exact values of this analysis are reproducible.

```{r}
# load needed packages. make sure they are installed.
pacman::p_load(here, #for data loading/saving
               tidyverse, # for data manipulation and plotting
               skimr, # for data exploration and summary
               ggthemes, # for plot theming
               tidytext, # for text analysis
               quanteda, # for text analysis
               caret, # for model training
               e1071, # for model training
               randomForest, # for model training
               gridExtra,
               irlba, # for SVD
               pROC, # for ROC curves
               doSNOW, # for parallel processing
               doParallel, # for parallel processing
               parallel) # for parallel processing

# load model data and test data frames
# Random Forest Models
rf.body.svd <- readRDS(here("results", "large-files", "rf-body-svd.rds"))
rf.body.svd.length <- readRDS(here("results", "large-files", "rf-body-svd-length.rds"))
rf.sub.svd <- readRDS(here("results", "large-files", "rf-sub-svd.rds"))
rf.sub.svd.length <- readRDS(here("results", "large-files", "rf-sub-svd-length.rds"))
rf.all.svd <- readRDS(here("results", "large-files", "rf-all-svd.rds"))
rf.all.svd.length <- readRDS(here("results", "large-files", "rf-all-svd-length.rds"))

# SVM Models
svm.body.svd <- readRDS(here("results", "large-files", "svm-body-svd.rds"))
svm.body.svd.length <- readRDS(here("results", "large-files", "svm-body-svd-length.rds"))
svm.sub.svd <- readRDS(here("results", "large-files", "svm-sub-svd.rds"))
svm.sub.svd.length <- readRDS(here("results", "large-files", "svm-sub-svd-length.rds"))
svm.all.svd <- readRDS(here("results", "large-files", "svm-all-svd.rds"))
svm.all.svd.length <- readRDS(here("results", "large-files", "svm-all-svd-length.rds"))

# XGBoost Models
xg.body.svd <- readRDS(here("results", "large-files", "xg-body-svd.rds"))
xg.body.svd.length <- readRDS(here("results", "large-files", "xg-body-svd-length.rds"))
xg.sub.svd <- readRDS(here("results", "large-files", "xg-sub-svd.rds"))
xg.sub.svd.length <- readRDS(here("results", "large-files", "xg-sub-svd-length.rds"))
xg.all.svd <- readRDS(here("results", "large-files", "xg-all-svd.rds"))
xg.all.svd.length <- readRDS(here("results", "large-files", "xg-all-svd-length.rds"))

# test data frames
test.body.svd <- readRDS(here("results", "large-files", "test-body-svd.rds"))
test.body.svd.length <- readRDS(here("results", "large-files", "test-body-svd-length.rds"))
test.sub.svd <- readRDS(here("results", "large-files", "test-sub-svd.rds"))
test.sub.svd.length <- readRDS(here("results", "large-files", "test-sub-svd-length.rds"))
test.all.svd <- readRDS(here("results", "large-files", "test-all-svd.rds"))
test.all.svd.length <- readRDS(here("results", "large-files", "test-all-svd-length.rds"))


# set global seed for reproducibility of exact values
set.seed(490)
```

## Generating Predictions on the Test Set

Then we'll create predictions for the test data with our tuned models and generate confusion matrices for each to evaluate the overall performance of each. We'll use this information to help inform which models generalized best when classifying new data and will assess which types of mistakes the models tend to err on the side of -- for our purposes allowing the occasional spam message through is likely better than overzealously blocking legitimate emails from reaching the user's inbox.

In order to determine the models least likely of overzealously blocking legitimate emails we'll consider evaluating model performance with either Recall or F1 score The former ensures we're classifying as many true legitimate emails correctly as possible, while the latter assesses the overall accuracy of our classification while factoring in the weights of any class imbalance. 

```{r}
# create all test predictions
# Random Forest Models
rf.body.svd.preds <- predict(rf.body.svd, test.body.svd)
rf.body.svd.length.preds <- predict(rf.body.svd.length, test.body.svd.length)
rf.sub.svd.preds <- predict(rf.sub.svd, test.sub.svd)
rf.sub.svd.length.preds <- predict(rf.sub.svd.length, test.sub.svd.length)
rf.all.svd.preds <- predict(rf.all.svd, test.all.svd)
rf.all.svd.length.preds <- predict(rf.all.svd.length, test.all.svd.length)

# SVM Models
svm.body.svd.preds <- predict(svm.body.svd, test.body.svd)
svm.body.svd.length.preds <- predict(svm.body.svd.length, test.body.svd.length)
svm.sub.svd.preds <- predict(svm.sub.svd, test.sub.svd)
svm.sub.svd.length.preds <- predict(svm.sub.svd.length, test.sub.svd.length)
svm.all.svd.preds <- predict(svm.all.svd, test.all.svd)
svm.all.svd.length.preds <- predict(svm.all.svd.length, test.all.svd.length)

# XGBoost Models
xg.body.svd.preds <- predict(xg.body.svd, test.body.svd)
xg.body.svd.length.preds <- predict(xg.body.svd.length, test.body.svd.length)
xg.sub.svd.preds <- predict(xg.sub.svd, test.sub.svd)
xg.sub.svd.length.preds <- predict(xg.sub.svd.length, test.sub.svd.length)
xg.all.svd.preds <- predict(xg.all.svd, test.all.svd)
xg.all.svd.length.preds <- predict(xg.all.svd.length, test.all.svd.length)
```

```{r}
# generate confusion matrices
# Random Forest Models
rf.body.svd.cm <- confusionMatrix(rf.body.svd.preds, test.body.svd$label)
rf.body.svd.length.cm <- confusionMatrix(rf.body.svd.length.preds, test.body.svd.length$label)
rf.sub.svd.cm <- confusionMatrix(rf.sub.svd.preds, test.sub.svd$label)
rf.sub.svd.length.cm <- confusionMatrix(rf.sub.svd.length.preds, test.sub.svd.length$label)
rf.all.svd.cm <- confusionMatrix(rf.all.svd.preds, test.all.svd$label)
rf.all.svd.length.cm <- confusionMatrix(rf.all.svd.length.preds, test.all.svd.length$label)

# SVM Models
svm.body.svd.cm <- confusionMatrix(svm.body.svd.preds, test.body.svd$label)
svm.body.svd.length.cm <- confusionMatrix(svm.body.svd.length.preds, test.body.svd.length$label)
svm.sub.svd.cm <- confusionMatrix(svm.sub.svd.preds, test.sub.svd$label)
svm.sub.svd.length.cm <- confusionMatrix(svm.sub.svd.length.preds, test.sub.svd.length$label)
svm.all.svd.cm <- confusionMatrix(svm.all.svd.preds, test.all.svd$label)
svm.all.svd.length.cm <- confusionMatrix(svm.all.svd.length.preds, test.all.svd.length$label)

# XGBoost Models
xg.body.svd.cm <- confusionMatrix(xg.body.svd.preds, test.body.svd$label)
xg.body.svd.length.cm <- confusionMatrix(xg.body.svd.length.preds, test.body.svd.length$label)
xg.sub.svd.cm <- confusionMatrix(xg.sub.svd.preds, test.sub.svd$label)
xg.sub.svd.length.cm <- confusionMatrix(xg.sub.svd.length.preds, test.sub.svd.length$label)
xg.all.svd.cm <- confusionMatrix(xg.all.svd.preds, test.all.svd$label)
xg.all.svd.length.cm <- confusionMatrix(xg.all.svd.length.preds, test.all.svd.length$label)

# create a data frame of combined confusion metrics for each model
cm.df <- rbind(rf.body.svd = rf.body.svd.cm$byClass,
               rf.body.svd.length = rf.body.svd.length.cm$byClass,
               rf.sub.svd = rf.sub.svd.cm$byClass,
               rf.sub.svd.length = rf.sub.svd.length.cm$byClass,
               rf.all.svd = rf.all.svd.cm$byClass,
               rf.all.svd.length = rf.all.svd.length.cm$byClass,
               svm.body.svd = svm.body.svd.cm$byClass,
               svm.body.svd.length = svm.body.svd.length.cm$byClass,
               svm.sub.svd = svm.sub.svd.cm$byClass,
               svm.sub.svd.length = svm.sub.svd.length.cm$byClass,
               svm.all.svd = svm.all.svd.cm$byClass,
               svm.all.svd.length = svm.all.svd.length.cm$byClass,
               xg.body.svd = xg.body.svd.cm$byClass,
               xg.body.svd.length = xg.body.svd.length.cm$byClass,
               xg.sub.svd = xg.sub.svd.cm$byClass,
               xg.sub.svd.length = xg.sub.svd.length.cm$byClass,
               xg.all.svd = xg.all.svd.cm$byClass,
               xg.all.svd.length = xg.all.svd.length.cm$byClass)
cm.df <- as.data.frame(cm.df) %>% 
          rownames_to_column(var = "Model")
saveRDS(cm.df, here("results", "tables", "confusion-matrix-compared.rds"))
```

Taking a look at the model permutation with the best F1 score and the best Recall we can see that in both cases they are those models utilizing only email body tokens for predictive power. Between the two models the XGBoost model manages a more impressive F1 score and a nearly identical Recall as the radial SVM model. As a result we'll consider the XGBoost model with only email body tokens our winner.

```{r}
# plot F1 scores
f1Plot <- cm.df %>% 
            mutate(Model = as.factor(Model),
                   F1 = as.numeric(F1)) %>% 
            ggplot(aes(F1, fct_reorder(Model, F1))) +
            geom_col(fill = "aquamarine3") +
            theme_tufte() +
            labs(title = "F1 Scores Compared") +
            ylab(NULL) + xlab("F1 Score") +
            theme(plot.title = element_text(hjust = 0.5, size = 18))
f1Plot

# plot Recall scores
recallPlot <- cm.df %>% 
            mutate(Model = as.factor(Model),
                   F1 = as.numeric(Recall)) %>% 
            ggplot(aes(F1, fct_reorder(Model, Recall))) +
            geom_col(fill = "aquamarine3") +
            theme_tufte() +
            labs(title = "Recall Compared") +
            ylab(NULL) + xlab("Recall") +
            theme(plot.title = element_text(hjust = 0.5, size = 18))
recallPlot

# model with the highest F1
cm.df %>% filter(F1 == max(F1))
# model with the highest recall
cm.df %>% filter(Recall == max(Recall))
```

We can now start to visualize some of the features of our best model to understand what helped it achieve such high predictive scores. Taking a look first at the confusion matrix for our two class model we can see that there were only 18 false negatives and only 26 false positives that would have slipped through to the user.

Taking a look at variable importance we begin to see the pitfalls of SVD on model interpretability. While we there are sharp declines in importance after the first two singular values and the second two singular values we're unable to ascertain what actual word choices in the body of the email these singular values are comprised of. However, considering the swift decline in importance it may be worth attempting to remodel using this same data without SVD on more powerful hardware so that better intrepretability might be possible. 

We do note that none of the highest performing models utilized either email subject tokens or email body length to achieve their scores. It's possible that the former only worked to introduce noise to the classification models and the latter was partially encompassed in the analysis of the tokens that comprise the body's length.
 
```{r}
# best tuning parameters for the best model
best.params <- t(xg.body.svd$bestTune)
colnames(best.params) <- "Value"
best.params <- as.data.frame(best.params) %>%
                rownames_to_column(var = "Parameter")
best.params
saveRDS(best.params, here("results", "tables", "best-params.rds"))

# print confusion matrix for best model
cm.best <- xg.body.svd.cm$table
cm.best

saveRDS(cm.best, here("results", "tables", "best-confusion-matrix.rds"))

# predict probabilities for best model
proba <- predict(xg.body.svd, test.body.svd, type = "prob")
curveROC <- roc(test.body.svd$label, proba[, 2], levels = rev(levels(test.body.svd$label)))
curveROC
auc_val <- auc(curveROC)

par(pty = "s")
plot.roc(curveROC, legacy.axes = T, percent = T, xlab = "False Positive Rate", ylab = "True Positive Rate", col = "aquamarine3", lwd = 5, print.auc = T)

saveRDS(curveROC, here("results", "tables", "roc.rds"))

# variable importance for best model
varImpPlot <- as.data.frame(varImp(xg.body.svd, scale = F)$importance) %>% 
                rownames_to_column(var = "Variable") %>% 
                mutate(Variable = as.factor(Variable)) %>% 
                slice_max(n = 20, order_by = Overall) %>% 
                ggplot(aes(y = Overall, x = fct_reorder(Variable, desc(Overall)))) +
                geom_col(fill = "aquamarine3") +
                theme_tufte() +
                labs(title = "Variable Importance") +
                ylab(NULL) + xlab(NULL) +
                theme(plot.title = element_text(hjust = 0.5, size = 18))
varImpPlot
ggsave(here("results", "figures", "final-var-imp.png"), varImpPlot)
```


