---
title: "Statistical Analysis"
format: html
---

# Prepping Train Data for Modeling

## Loading Data and Packages

Our first step is to import our processed data set and load in all necessary packages and functions for the analysis. Additionally we'll set the seed for the entirety of this document for the purposes of ensuring the exact values of this analysis are reproducible.

```{r}
# load needed packages. make sure they are installed.
pacman::p_load(here, #for data loading/saving
               tidyverse, # for data manipulation and plotting
               skimr, # for data exploration and summary
               ggthemes, # for plot theming
               tidytext, # for text analysis
               quanteda, # for text analysis
               caret, # for model training
               e1071, # for model training
               randomForest, # for model training
               irlba, # for SVD
               doSNOW, # for parallel processing
               doParallel, # for parallel processing
               parallel) # for parallel processing

# path to data
data_location <- here::here("data","processed-data","processeddata.rds")
# load data
mydata <- readRDS(data_location)

# set global seed for reproducibility of exact values
set.seed(490)
```

## Train/Test Splits

Next we'll focus on splitting the data into train and test splits. We'll opt to create a partition that leaves us 30% of the processed data set for testing the generalizability of our final model. We'll also want to ensure that class representations are maintained proportionate to the original data set.

```{r}
# create smaller partition for the sake of being able to run this shit on my PC
subsample <- createDataPartition(mydata$label,
                                 times = 1,
                                 p = .1,
                                 list = FALSE)

sub.data <- mydata[subsample,]

# create index of partition
inTrain <- createDataPartition(sub.data$label, 
                               times = 1, 
                               p = 0.7,
                               list = FALSE)

# subset training data
train <- sub.data[inTrain,]

# subset test data
test <- sub.data[-inTrain,]
```

The output of our class proportions look correctly maintained by the `createDataPartition()` function so we can proceed with preparing our training data for future models.

## Controls and Functions

```{r}
# create CV folds and model controls
folds <- createMultiFolds(train$label,
                          k = 10,
                          times = 3)
ctrl <- trainControl(method = "repeatedcv",
                     number = 10,
                     repeats = 3,
                     index = folds)
gridSVM <- expand.grid(C = seq(.1, 1, by = .1),
                       sigma = c(0.01, 0.1, 0.5, 1))

# function definitions
# calculate term frequency
term.freq <- function(row) {
  row / sum(row)
}

# calculate inverse document frequency
inverse.doc.freq <- function(col) {
  corpus.size <- length(col)
  doc.count <- length(which(col > 0))
  
  log10(corpus.size / doc.count)
}

# calculate tf-idf
tf.idf <- function(tf, idf) {
  tf * idf
}

# project singular value decomp onto test split
projSVD <- function(trainirlba, testtfidf) {
  sigma.inverse <- 1 / trainirlba$d
  u.transpose <- t(trainirlba$u)
  projection <- t(sigma.inverse * u.transpose %*% t(testtfidf))
  projection <- as.data.frame(projection)
  names(projection) <- paste0("X", seq(1,300))
  return(projection)
}

# tune Random Forest model
customRF <- function(df) {
  cores <- detectCores()
  clus <- makeCluster(cores[1] - 1) # determine the optimal number
                                                   # of cores for parallel processing
  registerDoParallel(clus)
  model <- train(label ~ ., 
                 data = df,
                 method = "rf",
                 trControl = ctrl,
                 tuneLength = 7)
  stopCluster(clus)
  return(model)
}

# tune an SVM model
customSVM <- function(df) {
  cores <- detectCores()
  clus <- makeCluster(cores[1] - 1) # determine the optimal number
                                                   # of cores for parallel processing
  registerDoParallel(clus)
  model <- train(label ~ ., 
                 data = df,
                 method = "svmRadial",
                 trControl = ctrl,
                 tuneLength = 7,
                 tuneGrid = gridSVM)
  stopCluster(clus)
  return(model)
}
```

To reduce repetitive model configurations and parameter tuning grids we'll establish some variables and functions for later use in the analysis.

## Body Tokens and SVD

```{r}
# generate tokens from email bodies
train.body.tokens <- tokens(train$body, what = "word",
                       remove_numbers = TRUE,
                       remove_separators = TRUE,
                       remove_punct = TRUE,
                       remove_symbols = TRUE,
                       split_hyphens = FALSE,
                       split_tags = TRUE,
                       remove_url = FALSE)
```

Our first step in structuring text data requires that we tokenize the email bodies.

```{r}
# lowercase tokens
train.body.tokens <- tokens_tolower(train.body.tokens)

# remove stopwords
train.body.tokens <- tokens_select(train.body.tokens,
                                   stopwords(),
                                   selection = "remove")

# stem words
train.body.tokens <- tokens_wordstem(train.body.tokens,
                                     language = "english")
```

After which we'll proceed with standardizing and consolidating the tokens. In our case we'll be lowercasing all tokens, removing English stopwords, and consolidating the semantics by stemming the remaining tokens.

```{r}
# create document-feature matrix
train.body.tokens.dfm <- dfm(train.body.tokens,
                             tolower = FALSE)

# convert dfm to matrix
train.body.tokens.matrix <- as.matrix(train.body.tokens.dfm)
```

After we've tokenized we're now able to restructure the data as a document-feature matrix in preparation for calculating term frequencies and inverse document frequencies.

```{r}
# create the term frequency
train.body.tokens.tf <- apply(train.body.tokens.matrix, 1, term.freq)

# create the inverse document frequency
train.body.tokens.idf <- apply(train.body.tokens.matrix, 2, inverse.doc.freq)

# create the tf-idf
train.body.tokens.tfidf <- apply(train.body.tokens.tf, 2, tf.idf, idf = train.body.tokens.idf)
```

We proceed by calculating TF-IDFs for the email body token data. This is valuable because it standardizes the word occurrences and further helps to reduce the influence of over represented words on the model. The TF values will accomplish the former while the IDF values will accomplish the latter by discounting the tokens found consistently throughout the corpus of emails. In the case of tokens found in every email throughout the corpus the end result is effectively complete removal from the model akin to our earlier removal of stop words.

```{r}
# transpose tf-idf and handle incomplete cases
train.body.tokens.tfidf <- t(train.body.tokens.tfidf)
incomplete.cases <- which(!complete.cases(train.body.tokens.tfidf))
train.body.tokens.tfidf[incomplete.cases,] <- rep(0.0, ncol(train.body.tokens.tfidf))
```

Having reinterpreted our tokens as TF-IDFs we can transpose the matrix back to it's original orientation and fill in zeroes for any tokens that generated NA values (as a result of no representation of the given tokens within the email).

```{r}
# run singular value decomposition for body tokens
train.body.irlba <- irlba(t(train.body.tokens.tfidf), nv = 300, maxit = 600)

saveRDS(train.body.irlba, file = here("results", "large-files", "train-body-irlba.rds"))
```

Finally, we're left with a matrix containing 40,399 features. While many classification models can ultimately accommodate such intense dimensionality, the computational resource requirements are quite high and would therefore make it unfeasible to run these models with the limited memory and processing power available on consumer-grade hardware. Additionally, the sparsity of our matrix (\>99.9%) suggests that many of the features will not be particularly predictive of either classification on their own.

As a result, we'll attempt to reduce the dimensionality of our feature set by employing the `irlba` package to perform singular value decomposition with a target feature limit of the 300 most informative predictor combinations.

## Subject Tokens and SVD

*This section features the same steps as above, but now for the email subject text. Justifications for each process will be the same as before.*

```{r}
# generate tokens from email subjects
train.sub.tokens <- tokens(train$subject, what = "word",
                       remove_numbers = TRUE,
                       remove_separators = TRUE,
                       remove_punct = TRUE,
                       remove_symbols = TRUE,
                       split_hyphens = FALSE,
                       split_tags = TRUE,
                       remove_url = FALSE)
```

```{r}
# lowercase tokens
train.sub.tokens <- tokens_tolower(train.sub.tokens)

# remove stopwords
train.sub.tokens <- tokens_select(train.sub.tokens,
                                   stopwords(),
                                   selection = "remove")

# stem words
train.sub.tokens <- tokens_wordstem(train.sub.tokens,
                                     language = "english")
```

```{r}
# create document-feature matrix
train.sub.tokens.dfm <- dfm(train.sub.tokens,
                             tolower = FALSE)
```

```{r}
# convert dfm to matrix
train.sub.tokens.matrix <- as.matrix(train.sub.tokens.dfm)
```

```{r}
# create the term frequency
train.sub.tokens.tf <- apply(train.sub.tokens.matrix, 1, term.freq)
```

```{r}
# create the inverse document frequency
train.sub.tokens.idf <- apply(train.sub.tokens.matrix, 2, inverse.doc.freq)
```

```{r}
# create the tf-idf
train.sub.tokens.tfidf <- apply(train.sub.tokens.tf, 2, tf.idf, idf = train.sub.tokens.idf)
```

```{r}
# transpose tf-idf and handle incomplete cases
train.sub.tokens.tfidf <- t(train.sub.tokens.tfidf)
incomplete.cases <- which(!complete.cases(train.sub.tokens.tfidf))
train.sub.tokens.tfidf[incomplete.cases,] <- rep(0.0, ncol(train.sub.tokens.tfidf))
```

```{r}
# create single value decomp for subject tokens
train.sub.irlba <- irlba(t(train.sub.tokens.tfidf), nv = 300, maxit = 600)

saveRDS(train.sub.irlba, file = here("results", "large-files", "train-sub-irlba.rds"))
```

## Final Data Frames for Modeling

```{r}
####### Body Token Only #######
# create revised training data frame with body token SVD
train.body.svd <- data.frame(label = train$label,
                             train.body.irlba$v)
saveRDS(train.body.svd, file = here("results", "large-files", "train-body-svd.rds"))

# create revised training data frame with body token SVD and email length
train.body.svd.length <- data.frame(label = train$label,
                               length = train$bodylength,
                               train.body.irlba$v)
saveRDS(train.body.svd.length, file = here("results", "large-files", "train-body-svd-length.rds"))

# create revised training data frame 
# with body token SVD, email length, and sender domain
train.body.svd.length.sender <- data.frame(label = train$label,
                                           length = train$bodylength,
                                           senderdomain = train$senderdomain,
                                           train.body.irlba$v)
saveRDS(train.body.svd.length.sender, file = here("results", "large-files", "train-body-svd-length-sender.rds"))

####### Subject Token Only #######
# create revised training data frame with subject token SVD
train.sub.svd <- data.frame(label = train$label,
                            train.sub.irlba$v)
saveRDS(train.sub.svd, file = here("results", "large-files", "train-sub-svd.rds"))

# create revised training data frame with subject token SVD and email length
train.sub.svd.length <- data.frame(label = train$label,
                                   length = train$bodylength,
                                   train.sub.irlba$v)
saveRDS(train.sub.svd.length, file = here("results", "large-files", "train-sub-svd-length.rds"))

# create revised training data frame 
# with subject token SVD, email length, and sender domain
train.sub.svd.length.sender <- data.frame(label = train$label,
                                          length = train$bodylength,
                                          senderdomain = train$senderdomain,
                                          train.sub.irlba$v)
saveRDS(train.sub.svd.length.sender, file = here("results", "large-files", "train-sub-svd-length-sender.rds"))

####### All Tokens #######
# create revised training data frame with all tokens SVD
train.all.svd <- data.frame(label = train$label,
                            train.body.irlba$v,
                            train.sub.irlba$v)
saveRDS(train.all.svd, file = here("results", "large-files", "train-all-svd.rds"))

# create revised training data frame with all tokens SVD and email length
train.all.svd.length <- data.frame(label = train$label,
                                   length = train$bodylength,
                                   train.body.irlba$v,
                                   train.sub.irlba$v)
saveRDS(train.all.svd.length, file = here("results", "large-files", "train-all-svd-length.rds"))

# create revised training data frame 
# with all tokens SVD, email length, and sender domain
train.all.svd.length.sender <- data.frame(label = train$label,
                                          length = train$bodylength,
                                          senderdomain = train$senderdomain,
                                          train.body.irlba$v,
                                          train.sub.irlba$v)
saveRDS(train.all.svd.length.sender, file = here("results", "large-files", "train-all-svd-length-sender.rds"))
```

Having generated structured text data that is also manageable, we can now piece together different data frames consisting of the various feature combinations we would like to assess for modeling. Based on the relationships observed in our exploratory data analysis those features are the token data for both email subject and body, the body length of the email, and the sender domain for the email.

# Tune Models

## Random Forest Iterations

```{r}
###### Body Tokens Only ######
# tune a Random Forest model with body token SVD
rf.body.svd <- customRF(train.body.svd)

saveRDS(rf.body.svd, file = here("results", "large-files", "rf-body-svd.rds"))

# tune a Random Forest model with body token SVD and email length
rf.body.svd.length <- customRF(train.body.svd.length)

saveRDS(rf.body.svd.length, file = here("results", "large-files", "rf-body-svd-length.rds"))

### can't be run in a reasonable timeframe
### too computationally intensive for a MacBook
# tune a Random Forest model with body token SVD, email length, and sender domain
#rf.body.svd.length.sender <- customRF(train.body.svd.length.sender)

#saveRDS(rf.body.svd.length.sender, file = here("results", "large-files", "rf-body-svd-length-sender.rds"))

###### Subject Tokens Only ######
# tune a Random Forest model with subject token SVD
rf.sub.svd <- customRF(train.sub.svd)

saveRDS(rf.sub.svd, file = here("results", "large-files", "rf-sub-svd.rds"))

# tune a Random Forest model with subject token SVD and email length
rf.sub.svd.length <- customRF(train.sub.svd.length)

saveRDS(rf.sub.svd.length, file = here("results", "large-files", "rf-sub-svd-length.rds"))

### can't be run in a reasonable timeframe
### too computationally intensive for a MacBook
# tune a Random Forest model with subject token SVD, email length, and sender domain
#rf.sub.svd.length.sender <- customRF(train.sub.svd.length.sender)

#saveRDS(rf.sub.svd.length.sender, file = here("results", "large-files", "rf-sub-svd-length-sender.rds"))

###### All Tokens ######
# tune a Random Forest model with all token SVD
rf.all.svd <- customRF(train.all.svd)

saveRDS(rf.all.svd, file = here("results", "large-files", "rf-all-svd.rds"))

# tune a Random Forest model with all token SVD and email length
rf.all.svd.length <- customRF(train.all.svd.length)

saveRDS(rf.all.svd.length, file = here("results", "large-files", "rf-all-svd-length.rds"))

### can't be run in a reasonable timeframe
### too computationally intensive for a MacBook
# tune a Random Forest model with all token SVD, email length, and sender domain
#rf.all.svd.length.sender <- customRF(train.all.svd.length.sender)

#saveRDS(rf.all.svd.length.sender, file = here("results", "large-files", "rf-all-svd-length-sender.rds"))
```

## SVM Iterations

```{r}
###### Body Tokens Only ######
# tune an SVM model with body token SVD
svm.body.svd <- customSVM(train.body.svd)

saveRDS(svm.body.svd, file = here("results", "large-files", "svm-body-svd.rds"))

# tune an SVM model with body token SVD and email length
svm.body.svd.length <- customSVM(train.body.svd.length)

saveRDS(svm.body.svd.length, file = here("results", "large-files", "svm-body-svd-length.rds"))

### can't be run in a reasonable timeframe
### too computationally intensive for a MacBook
# tune an SVM model with body token SVD, email length, and sender domain
#svm.body.svd.length.sender <- customSVM(train.body.svd.length.sender)

#saveRDS(svm.body.svd.length.sender, file = here("results", "large-files", "svm-body-svd-length-sender.rds"))

###### Subject Tokens Only ######
# tune an SVM model with subject token SVD
svm.sub.svd <- customSVM(train.sub.svd)

saveRDS(svm.sub.svd, file = here("results", "large-files", "svm-sub-svd.rds"))

# tune an SVM model with subject token SVD and email length
svm.sub.svd.length <- customSVM(train.sub.svd.length)

saveRDS(svm.sub.svd.length, file = here("results", "large-files", "svm-sub-svd-length.rds"))

### can't be run in a reasonable timeframe
### too computationally intensive for a MacBook
# tune an SVM model with subject token SVD, email length, and sender domain
#svm.sub.svd.length.sender <- customSVM(train.sub.svd.length.sender)

#saveRDS(svm.sub.svd.length.sender, file = here("results", "large-files", "svm-sub-svd-length-sender.rds"))

###### All Tokens ######
# tune an SVM model with all token SVD
svm.all.svd <- customSVM(train.all.svd)

saveRDS(svm.all.svd, file = here("results", "large-files", "svm-all-svd.rds"))

# tune an SVM model with all token SVD and email length
svm.all.svd.length <- customSVM(train.all.svd.length)

saveRDS(svm.all.svd.length, file = here("results", "large-files", "svm-all-svd-length.rds"))

### can't be run in a reasonable timeframe
### too computationally intensive for a MacBook
# tune an SVM model with all token SVD, email length, and sender domain
#svm.all.svd.length.sender <- customSVM(train.all.svd.length.sender)

#saveRDS(svm.all.svd.length.sender, file = here("results", "large-files", "svm-all-svd-length-sender.rds"))
```

# Prepping Test Data for Predictions

## Body Tokens and SVD

Having now run and saved our respective iterations of various models and features we can now proceed with preparing the test data so that it exists in the same features space with the same data transformations. The first step of which is to follow tokenization utilizing the same exact methods as we did for the training data. 

```{r}
# generate tokens from test email bodies
test.body.tokens <- tokens(test$body, what = "word",
                       remove_numbers = TRUE,
                       remove_separators = TRUE,
                       remove_punct = TRUE,
                       remove_symbols = TRUE,
                       split_hyphens = FALSE,
                       split_tags = TRUE,
                       remove_url = FALSE)
```

```{r}
# lowercase tokens
test.body.tokens <- tokens_tolower(test.body.tokens)

# remove stopwords
test.body.tokens <- tokens_select(test.body.tokens,
                                   stopwords(),
                                   selection = "remove")

# stem words
test.body.tokens <- tokens_wordstem(test.body.tokens,
                                     language = "english")
```

We begin to deviate from our processing workflow utilized in the training section in order to ensure that the data is mapped into the same feature space as the training data used for modeling. In order to ensure this we'll utilize the `dfm_match()` function to project the same token space from the training data onto our test data.

```{r}
# create document-feature matrix
test.body.tokens.dfm <- dfm(test.body.tokens,
                             tolower = FALSE)

# project the feature set from the train split onto the test split
test.body.tokens.dfm <- dfm_match(test.body.tokens.dfm, features = featnames(train.body.tokens.dfm))

# convert dfm to matrix
test.body.tokens.matrix <- as.matrix(test.body.tokens.dfm)
```

Similarly we need to ensure the inverse document frequency values used to discount overrepresented terms are the same as those utilized in our training data when tranforming to TF-IDFs. 

```{r}
test.body.tokens.tf <- apply(test.body.tokens.matrix, 1, term.freq)

test.body.tokens.tfidf <- apply(test.body.tokens.tf, 2, tf.idf, idf = train.body.tokens.idf)

# transpose and fix missing values
test.body.tokens.tfidf <- t(test.body.tokens.tfidf)
test.body.tokens.tfidf[is.na(test.body.tokens.tfidf)] <- 0.0
```

Finally we'll be using the same SVD process as was used on the training data. We'll project the data using our custom `projSVD()` function defined in the Controls and Functions section above.

```{r}
# project the SVD from the train split onto the test split
test.body.svd.proj <- projSVD(train.body.irlba, test.body.tokens.tfidf)
```

## Subject Tokens and SVD

```{r}
# generate tokens from test email bodies
test.sub.tokens <- tokens(test$subject, what = "word",
                          remove_numbers = TRUE,
                          remove_separators = TRUE,
                          remove_punct = TRUE,
                          remove_symbols = TRUE,
                          split_hyphens = FALSE,
                          split_tags = TRUE,
                          remove_url = FALSE)
```

```{r}
# lowercase tokens
test.sub.tokens <- tokens_tolower(test.sub.tokens)

# remove stopwords
test.sub.tokens <- tokens_select(test.sub.tokens,
                                 stopwords(),
                                 selection = "remove")

# stem words
test.sub.tokens <- tokens_wordstem(test.sub.tokens,
                                   language = "english")
```

```{r}
# create document-feature matrix
test.sub.tokens.dfm <- dfm(test.sub.tokens,
                           tolower = FALSE)
```

```{r}
# project the feature set from the train split onto the test split
test.sub.tokens.dfm <- dfm_match(test.sub.tokens.dfm, features = featnames(train.sub.tokens.dfm))
```

```{r}
# convert dfm to matrix
test.sub.tokens.matrix <- as.matrix(test.sub.tokens.dfm)
```

```{r}
test.sub.tokens.tf <- apply(test.sub.tokens.matrix, 1, term.freq)
```

```{r}
test.sub.tokens.tfidf <- apply(test.sub.tokens.tf, 2, tf.idf, idf = train.sub.tokens.idf)
```

```{r}
# transpose and fix missing values
test.sub.tokens.tfidf <- t(test.sub.tokens.tfidf)
test.sub.tokens.tfidf[is.na(test.sub.tokens.tfidf)] <- 0.0
```

```{r}
# project the SVD from the train split onto the test split
test.sub.svd.proj <- projSVD(train.sub.irlba, test.sub.tokens.tfidf)
```

## Final Data Frames for Predictions

```{r}
####### Body Token Only #######
# create data frame of body tokens and labels
test.body.svd <- cbind(label = test$label,
                       test.body.svd.proj)

saveRDS(test.body.svd, file = here("results", "large-files", "test-body-svd.rds"))

# create data frame of body tokens and labels
test.body.svd.length <- cbind(label = test$label,
                              length = test$bodylength,
                              test.body.svd.proj)

saveRDS(test.body.svd.length, file = here("results", "large-files", "test-body-svd-length.rds"))

# create data frame of body tokens and labels
test.body.svd.length.sender <- cbind(label = test$label,
                                     length = test$bodylength,
                                     senderdomain = test$senderdomain,
                                     test.body.svd.proj)

saveRDS(test.body.svd.length.sender, file = here("results", "large-files", "test-body-svd-length-sender.rds"))

####### Subject Token Only #######
# create data frame of subject tokens and labels
test.sub.svd <- cbind(label = test$label,
                      test.sub.svd.proj)

saveRDS(test.sub.svd, file = here("results", "large-files", "test-sub-svd.rds"))

# create data frame of subject tokens and labels
test.sub.svd.length <- cbind(label = test$label,
                             length = test$bodylength,
                             test.sub.svd.proj)

saveRDS(test.sub.svd.length, file = here("results", "large-files", "test-sub-svd-length.rds"))

# create data frame of subject tokens and labels
test.sub.svd.length.sender <- cbind(label = test$label,
                                    length = test$bodylength,
                                    senderdomain = test$senderdomain,
                                    test.sub.svd.proj)

saveRDS(test.sub.svd.length.sender, file = here("results", "large-files", "test-sub-svd-length-sender.rds"))

####### All Tokens #######
# create data frame of all tokens and labels
test.all.svd <- cbind(label = test$label,
                      test.body.svd.proj,
                      test.sub.svd.proj)

saveRDS(test.all.svd, file = here("results", "large-files", "test-all-svd.rds"))

# create data frame of all tokens and labels
test.all.svd.length <- cbind(label = test$label,
                             length = test$bodylength,
                             test.body.svd.proj,
                             test.sub.svd.proj)

saveRDS(test.all.svd.length, file = here("results", "large-files", "test-all-svd-length.rds"))

# create data frame of all tokens and labels
test.all.svd.length.sender <- cbind(label = test$label,
                                    length = test$bodylength,
                                    senderdomain = test$senderdomain,
                                    test.body.svd.proj,
                                    test.sub.svd.proj)

saveRDS(test.all.svd.length.sender, file = here("results", "large-files", "test-all-svd-length-sender.rds"))
```



The `echo: false` option disables the printing of code (only output is displayed).
